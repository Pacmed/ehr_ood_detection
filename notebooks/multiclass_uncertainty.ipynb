{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "DEFAULT_BATCH_SIZE = 2\n",
    "DEFAULT_EARLY_STOPPING_PAT = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some code for the models adjusted for multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModule(nn.Module):\n",
    "    \"\"\"Create a new multi-layer perceptron instance with ReLU activations and no non-linearity in\n",
    "    the output layer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hidden_sizes: list\n",
    "        The sizes of the hidden layers.\n",
    "    input_size: int\n",
    "        The input size.\n",
    "    dropout_rate: float\n",
    "        The dropout rate applied after each layer (except the output layer)\n",
    "    output_size: int\n",
    "        The output size.\n",
    "    do_batch_norm: bool\n",
    "        Whether to apply batch normalization after each layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_sizes: list, input_size: int, dropout_rate: float,\n",
    "                 output_size: int = 1, do_batch_norm: bool = False):\n",
    "        super(MLPModule, self).__init__()\n",
    "        layers = []\n",
    "\n",
    "        if len(hidden_sizes) > 0:\n",
    "            layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "            if do_batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(num_features=hidden_sizes[0]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        for i in range(1, len(hidden_sizes)):\n",
    "            layers.append(nn.Linear(hidden_sizes[i - 1], hidden_sizes[i]))\n",
    "            if do_batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(num_features=hidden_sizes[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, _input: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Perform a forward pass of the MLP.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        _input: torch.Tensor\n",
    "            The input of the model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        type: torch.Tensor\n",
    "            The output of the model.\n",
    "        \"\"\"\n",
    "        return self.mlp(_input)\n",
    "\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    \"\"\"Create a new (simple) PyTorch Dataset instance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: torch.Tensor\n",
    "        Predictors\n",
    "    y: torch.Tensor\n",
    "        Target\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X: torch.Tensor, y: torch.Tensor):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of items in the dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        type: int\n",
    "            The number of items in the dataset.\n",
    "        \"\"\"\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Return X and y at index idx.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx: int\n",
    "            Index.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        type: Tuple[torch.Tensor, torch.Tensor]\n",
    "            X and y at index idx\n",
    "        \"\"\"\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    \"\"\"Handles training of an MLPModule.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hidden_sizes: list\n",
    "        The sizes of the hidden layers.\n",
    "    input_size: int\n",
    "        The input size.\n",
    "    output_size: int\n",
    "        The output size.\n",
    "    dropout_rate: float\n",
    "        The dropout rate applied after each layer (except the output layer)\n",
    "    batch_norm: bool\n",
    "        Whether to apply batch normalization after each layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_sizes: list, input_size: int, dropout_rate: float,\n",
    "                 class_weight: bool = True, output_size: int = 1, batch_norm: bool = False):\n",
    "        self.model = MLPModule(hidden_sizes, input_size, dropout_rate, output_size, batch_norm)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters())\n",
    "        self.class_weight = class_weight\n",
    "\n",
    "    def _initialize_dataloader(self, X_train: np.ndarray, y_train: np.ndarray, batch_size: int):\n",
    "        \"\"\"Initialize the dataloader of the train data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train: np.ndarray\n",
    "            The training data.\n",
    "        y_train: np.ndarray\n",
    "            The labels corresponding to the training data.\n",
    "        batch_size:\n",
    "            The batch size.\n",
    "        \"\"\"\n",
    "        train_set = SimpleDataset(torch.from_numpy(X_train),\n",
    "                                  torch.from_numpy(y_train))\n",
    "        self.train_loader = DataLoader(train_set, batch_size, shuffle=True)\n",
    "\n",
    "    def validate(self, X_val: np.ndarray, y_val: np.ndarray) -> torch.Tensor:\n",
    "        \"\"\"Calculate the validation loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_val: np.ndarray\n",
    "            The validation data.\n",
    "        y_val: np.ndarray\n",
    "            The labels corresponding to the validation data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        type: torch.Tensor\n",
    "            The validation loss.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        y_pred = self.model(torch.tensor(X_val).float())\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        val_loss = loss_fn(y_pred, torch.tensor(y_val).long())\n",
    "        return val_loss\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray, X_val: np.ndarray, y_val: np.ndarray,\n",
    "              batch_size, n_epochs,\n",
    "              early_stopping: bool =False,\n",
    "              early_stopping_patience: int = DEFAULT_EARLY_STOPPING_PAT):\n",
    "        \"\"\"Train the MLP.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train: np.ndarray\n",
    "            The training data.\n",
    "        y_train: np.ndarray\n",
    "            The labels corresponding to the training data.\n",
    "        X_val: np.ndarray\n",
    "            The validation data.\n",
    "        y_val: np.ndarray\n",
    "            The labels corresponding to the validation data.\n",
    "        batch_size: int\n",
    "            The batch size, default 256\n",
    "        n_epochs: int\n",
    "            The number of training epochs, default 30\n",
    "        early_stopping: bool\n",
    "            Whether to perform early stopping, default True\n",
    "        early_stopping_patience: int\n",
    "            The early stopping patience, default 2.\n",
    "        \"\"\"\n",
    "\n",
    "        self._initialize_dataloader(X_train, y_train, batch_size)\n",
    "        prev_val_loss = float('inf')\n",
    "        n_no_improvement = 0\n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "            self.model.train()\n",
    "            for batch_X, batch_y in self.train_loader:\n",
    "                y_pred = self.model(batch_X.float())\n",
    "                loss_fn = torch.nn.CrossEntropyLoss()\n",
    "                loss = loss_fn(y_pred, batch_y.long())\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            if early_stopping:\n",
    "                val_loss = self.validate(X_val, y_val)\n",
    "                if val_loss > prev_val_loss:\n",
    "                    n_no_improvement += 1\n",
    "                else:\n",
    "                    n_no_improvement = 0\n",
    "                    prev_val_loss = val_loss\n",
    "            if n_no_improvement >= early_stopping_patience:\n",
    "                break\n",
    "\n",
    "class NNEnsemble:\n",
    "    \"\"\"Wrapper class for an ensemble of neural networks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_models: int\n",
    "        The number of ensemble members.\n",
    "    model_params: dict\n",
    "        The model parameters, see class MLP.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_models: int, model_params: dict):\n",
    "        self.n_models = n_models\n",
    "        self.model_params = model_params\n",
    "        self.models = dict()\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray, X_val: np.ndarray, y_val: np.ndarray,\n",
    "              training_params: dict):\n",
    "        \"\"\"Train all MLPs on the training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train: np.ndarray\n",
    "            The training data\n",
    "        y_train: np.ndarray\n",
    "            The labels corresponding to the training data.\n",
    "        X_val: np.ndarray\n",
    "            The validation data\n",
    "        y_val: np.ndarray\n",
    "            The labels corresponding to the validation data.\n",
    "        training_params: dict\n",
    "            The parameters used for training, see class MLP.\n",
    "        \"\"\"\n",
    "        for i in range(self.n_models):\n",
    "            mlp = MLP(**self.model_params)\n",
    "            mlp.train(X_train, y_train, X_val, y_val,\n",
    "                      **training_params)\n",
    "            self.models[i] = mlp.model\n",
    "\n",
    "    def predict_proba(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predict the probabilities p(y|X) by averaging the predictions of ensemble members.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_test: np.ndarray\n",
    "            The test data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        type:np.ndarray\n",
    "            The predicted probabilities.\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        X_test_tensor = torch.tensor(X_test).float()\n",
    "        for i in range(self.n_models):\n",
    "            self.models[i].eval()\n",
    "            predictions.append(torch.softmax(\n",
    "                self.models[i](X_test_tensor), dim=1).detach().squeeze().numpy())\n",
    "        return np.array(predictions).mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions that are used later to generate data and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cluster(mean, diag_cov, size, label):\n",
    "    c = np.random.multivariate_normal(mean=mean, cov=[[diag_cov, 0],[0,diag_cov]], size=size)\n",
    "    c = np.expand_dims(c, 2)\n",
    "    y = np.ones(shape=(c.shape[0],1,1))*label\n",
    "    c = np.concatenate([c, y], axis=1)\n",
    "    return c\n",
    "\n",
    "def plot_single_grid(X, y, xx, yy, grid, alpha=0.1, s=50, title='', uncertainty=True, \n",
    "                     colorbar=True, vmin=None, vmax=None):\n",
    "    if colorbar:\n",
    "        plt.figure(figsize=(6,5))\n",
    "    else:\n",
    "        plt.figure(figsize=(5,5))\n",
    "    if uncertainty:\n",
    "        cmap = plt.cm.Purples\n",
    "    else: \n",
    "        cmap = plt.cm.RdBu\n",
    "    c = plt.contourf(xx, yy, grid.reshape(xx.shape), cmap=cmap,\n",
    "                    levels=20, vmin=vmin, vmax=vmax)\n",
    "    if colorbar:\n",
    "        plt.colorbar(c)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y,cmap='Set1', \n",
    "                edgecolors='k', alpha=alpha, s=s)\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    \n",
    "def entropy(pk, axis):\n",
    "    return -np.sum(pk * np.log2(pk+1e-8), axis=axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ugly code to generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_size, negative_size = 50, 50 \n",
    "\n",
    "one_mean = np.array([ 0.5, -0.5])\n",
    "two_mean = np.array([0,-1.5])\n",
    "three_mean = np.array([-1.3,-1])\n",
    "four_mean = np.array([-1.5,0])\n",
    "five_mean = np.array([-0.1,0.5])\n",
    "six_mean = np.array([ -2.5, 0.5])\n",
    "seven_mean = np.array([ 1.5, 0])\n",
    "eight_mean = np.array([2, 1.1])\n",
    "nine_mean = np.array([2, -1.5])\n",
    "one_cluster = generate_cluster(one_mean, np.random.uniform(0.02,0.1), positive_size, label=0)\n",
    "two_cluster = generate_cluster(two_mean, np.random.uniform(0.02,0.1), negative_size, label=1)\n",
    "three_cluster = generate_cluster(three_mean, np.random.uniform(0.02,0.1), negative_size, label=2)\n",
    "four_cluster = generate_cluster(four_mean, np.random.uniform(0.02,0.1), negative_size, label=3)\n",
    "five_cluster = generate_cluster(five_mean, np.random.uniform(0.02,0.1), negative_size, label=4)\n",
    "six_cluster = generate_cluster(six_mean, np.random.uniform(0.02,0.1), negative_size, label=5)\n",
    "seven_cluster = generate_cluster(seven_mean, np.random.uniform(0.02,0.1), negative_size, label=6)\n",
    "eight_cluster = generate_cluster(eight_mean,np.random.uniform(0.02,0.1), negative_size, label=7)\n",
    "nine_cluster = generate_cluster(nine_mean,np.random.uniform(0.02,0.1), negative_size, label=8)\n",
    "concatenated = np.concatenate([one_cluster, two_cluster, three_cluster, four_cluster, \n",
    "                               five_cluster, six_cluster, seven_cluster, eight_cluster, nine_cluster], axis=0)\n",
    "\n",
    "columns = ['f1', 'f2']\n",
    "label = 'label'\n",
    "df_model = pd.DataFrame(concatenated[:,:,0], columns=columns+[label])\n",
    "df_model = sklearn.utils.shuffle(df_model, random_state=1)\n",
    "\n",
    "X_train = df_model[columns].values\n",
    "y_train = df_model[label].values\n",
    "\n",
    "step_size = .1 \n",
    "x_min, x_max = -3.5,3\n",
    "y_min, y_max = -2.3,2\n",
    "\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, step_size),\n",
    "                     np.arange(y_min, y_max, step_size))\n",
    "\n",
    "X_grid = np.c_[xx.ravel(), yy.ravel()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette(\"Set1\", 10)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1],  c=y_train, cmap='Set3',\n",
    "                edgecolors='k', alpha=0.9, s=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {'hidden_sizes': [5,5],\n",
    "                'dropout_rate': 0.0,\n",
    "                'input_size': 2,\n",
    "                 'output_size':9,\n",
    "                'batch_norm': False}\n",
    "\n",
    "training_params = {'batch_size':8,'n_epochs':100,\n",
    "                   'early_stopping':True}\n",
    "model = MLP(**model_params)\n",
    "model.train(X_train, y_train, X_train, y_train, **training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_ensemble = NNEnsemble(n_models=10, model_params=model_params)\n",
    "nn_ensemble.train(X_train, y_train, X_train, y_train, training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_predictions_ensemble = nn_ensemble.predict_proba(X_grid)\n",
    "grid_uncertainties_ensemble = entropy(grid_predictions_ensemble, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.eval()\n",
    "predictions = torch.softmax(model.model(torch.tensor(X_grid).float()), dim=1).detach().numpy()\n",
    "grid_uncertainties = entropy(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_grid(X_train, y_train, xx, yy, grid_uncertainties_ensemble, s=50, alpha=0.5,title='', uncertainty=True, \n",
    "                     colorbar=False)\n",
    "plt.savefig('multiclass_ensemble.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_grid(X_train, y_train, xx, yy, grid_uncertainties, s=50, title='', alpha=0.5, uncertainty=True, \n",
    "                     colorbar=False)\n",
    "plt.savefig('multiclass_single.png', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
